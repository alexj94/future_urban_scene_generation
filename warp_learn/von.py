from typing import Union

import cv2
import numpy as np
import torch
import torch.nn.functional as F
from PIL import Image
from torch import nn
from torchvision.transforms import ToTensor, Normalize

from utils.crop_utils import square_crop_from_bbox
from warp_learn.planes_utils import planes_to_torch


class LayerNorm(nn.Module):
    def __init__(self, num_features, eps=1e-5, affine=True):
        super(LayerNorm, self).__init__()
        self.num_features = num_features
        self.affine = affine
        self.eps = eps

        if self.affine:
            self.gamma = nn.Parameter(torch.Tensor(num_features).uniform_())
            self.beta = nn.Parameter(torch.zeros(num_features))

    def forward(self, x):
        shape = [-1] + [1] * (x.dim() - 1)
        mean = x.view(x.size(0), -1).mean(1).view(*shape)
        std = x.view(x.size(0), -1).std(1).view(*shape)
        x = (x - mean) / (std + self.eps)

        if self.affine:
            shape = [1, -1] + [1] * (x.dim() - 2)
            x = x * self.gamma.view(*shape) + self.beta.view(*shape)
        return x


class Conv2dBlock(nn.Module):
    def __init__(self, input_dim, output_dim, kernel_size, stride,
                 padding=0, norm='none', activation='relu', pad_type='zero'):
        super(Conv2dBlock, self).__init__()
        self.use_bias = True
        # initialize padding
        if pad_type == 'reflect':
            self.pad = nn.ReflectionPad2d(padding)
        elif pad_type == 'zero':
            self.pad = nn.ZeroPad2d(padding)
        else:
            assert 0, "Unsupported padding type: {}".format(pad_type)

        # initialize normalization
        norm_dim = output_dim
        if norm == 'batch':
            self.norm = nn.BatchNorm2d(norm_dim)
        elif norm == 'inst':
            self.norm = nn.InstanceNorm2d(norm_dim, track_running_stats=False)
        elif norm == 'ln':
            self.norm = LayerNorm(norm_dim)
        elif norm == 'none':
            self.norm = None
        else:
            assert 0, "Unsupported normalization: {}".format(norm)

        # initialize activation
        if activation == 'relu':
            self.activation = nn.ReLU(inplace=True)
        elif activation == 'lrelu':
            self.activation = nn.LeakyReLU(0.2, inplace=True)
        elif activation == 'prelu':
            self.activation = nn.PReLU()
        elif activation == 'selu':
            self.activation = nn.SELU(inplace=True)
        elif activation == 'tanh':
            self.activation = nn.Tanh()
        elif activation == 'none':
            self.activation = None
        else:
            assert 0, "Unsupported activation: {}".format(activation)

        # initialize convolution
        self.conv = nn.Conv2d(input_dim, output_dim, kernel_size, stride,
                              bias=self.use_bias)

    def forward(self, x):
        x = self.conv(self.pad(x))
        if self.norm:
            x = self.norm(x)
        if self.activation:
            x = self.activation(x)
        return x


class ResBlock(nn.Module):
    def __init__(self, dim, norm='inst', activation='relu', pad_type='zero',
                 nz=0):
        super(ResBlock, self).__init__()

        model = []
        model += [Conv2dBlock(dim + nz, dim, 3, 1, 1, norm=norm,
                              activation=activation, pad_type=pad_type)]
        model += [
            Conv2dBlock(dim, dim + nz, 3, 1, 1, norm=norm, activation='none',
                        pad_type=pad_type)]
        self.model = nn.Sequential(*model)

    def forward(self, x):
        residual = x
        out = self.model(x)
        out += residual
        return out


class ResBlocks(nn.Module):
    def __init__(self, num_blocks, dim, norm='inst', activation='relu',
                 pad_type='zero', nz=0):
        super(ResBlocks, self).__init__()
        self.model = []
        for i in range(num_blocks):
            self.model += [ResBlock(dim, norm=norm, activation=activation,
                                    pad_type=pad_type, nz=nz)]
        self.model = nn.Sequential(*self.model)

    def forward(self, x):
        return self.model(x)


class ContentEncoder(nn.Module):
    def __init__(self, n_downsample, n_res, input_dim, dim, norm, activ,
                 pad_type='zero'):
        super(ContentEncoder, self).__init__()
        self.model = []
        self.model += [
            Conv2dBlock(input_dim, dim, 7, 1, 3, norm=norm, activation=activ,
                        pad_type='reflect')]
        # downsampling blocks
        for i in range(n_downsample):
            self.model += [
                Conv2dBlock(dim, 2 * dim, 4, 2, 1, norm=norm, activation=activ,
                            pad_type='reflect')]
            dim *= 2
        # residual blocks
        self.model += [ResBlocks(n_res, dim, norm=norm, activation=activ,
                                 pad_type=pad_type)]
        self.model = nn.Sequential(*self.model)
        self.output_dim = dim

    def forward(self, x):
        return self.model(x)


class Upsample(nn.Module):
    def __init__(self, scale_factor, mode='nearest'):
        super().__init__()
        self.factor = scale_factor
        self.mode = mode

    def forward(self, x):
        return torch.nn.functional.interpolate(x, scale_factor=self.factor,
                                               mode=self.mode)


class Decoder(nn.Module):
    def __init__(self, n_upsample, n_res, dim, output_dim, norm='batch',
                 activ='relu', pad_type='zero', nz=0):
        super(Decoder, self).__init__()

        self.model = []
        # AdaIN residual blocks
        self.model += [
            ResBlocks(n_res, dim, norm, activ, pad_type=pad_type, nz=nz)]
        # upsampling blocks
        for i in range(n_upsample):
            if i == 0:
                input_dim = dim + nz
            else:
                input_dim = dim
            self.model += [Upsample(scale_factor=2),
                           Conv2dBlock(input_dim, dim // 2, 5, 1, 2, norm='ln',
                                       activation=activ, pad_type='reflect')]
            dim //= 2
        # use reflection padding in the last conv layer
        self.model += [Conv2dBlock(dim, output_dim, 7, 1, 3, norm='none',
                                   activation='tanh', pad_type='reflect')]
        self.model = nn.Sequential(*self.model)

    def forward(self, x):
        return self.model(x)


class G_Resnet(nn.Module):
    def __init__(self, input_nc, output_nc=3, num_downs=2, n_res=3, ngf=64,
                 norm='inst', nl_layer='relu'):
        super(G_Resnet, self).__init__()
        n_downsample = num_downs
        pad_type = 'reflect'
        self.enc_content = ContentEncoder(n_downsample, n_res, input_nc, ngf,
                                          norm, nl_layer, pad_type=pad_type)
        self.dec = Decoder(n_downsample, n_res, self.enc_content.output_dim,
                           output_nc, norm=norm, activ=nl_layer,
                           pad_type=pad_type, nz=0)

    def decode(self, content):
        return self.dec(content)

    def forward(self, image):
        content = self.enc_content(image)
        images_recon = self.decode(content)
        return images_recon


class D_NLayersMulti(nn.Module):
    def __init__(self, input_nc, ndf=64, n_layers=2, num_D=2):
        super(D_NLayersMulti, self).__init__()
        self.num_D = num_D
        layers = self.get_layers(input_nc, ndf, n_layers)
        self.add_module("model_0", nn.Sequential(*layers))
        self.down = nn.AvgPool2d(3, stride=2, padding=[1, 1], count_include_pad=False)
        for i in range(1, num_D):
            ndf_i = int(round(ndf / (2**i)))  # Just using ndf also works
            layers = self.get_layers(input_nc, ndf_i, n_layers)
            self.add_module("model_%d" % i, nn.Sequential(*layers))

    def get_layers(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.InstanceNorm2d):
        kw = 4
        padw = 1
        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]

        nf_mult = 1
        nf_mult_prev = 1
        for n in range(1, n_layers):
            nf_mult_prev = nf_mult
            nf_mult = min(2**n, 8)
            sequence += [
                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw),
                norm_layer(ndf * nf_mult),
                nn.LeakyReLU(0.2, True)
            ]

        nf_mult_prev = nf_mult
        nf_mult = min(2**n_layers, 8)
        sequence += [
            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw),
            norm_layer(ndf * nf_mult),
            nn.LeakyReLU(0.2, True)
        ]

        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]

        return sequence

    def forward(self, input):
        result = []
        down = input
        for i in range(self.num_D):
            model = getattr(self, "model_%d" % i)
            result.append(model(down))
            if i != self.num_D - 1:
                down = self.down(down)
        return result

class GANLoss(nn.Module):
    """Define different GAN objectives.
    The GANLoss class abstracts away the need to create the target label tensor
    that has the same size as the input.
    """

    def __init__(self, target_real_label=1.0, target_fake_label=0.0):
        """ Initialize the GANLoss class.
        Parameters:
            target_real_label (bool) - - label for a real image
            target_fake_label (bool) - - label of a fake image
        Note: Do not use sigmoid as the last layer of Discriminator.
        LSGAN needs no sigmoid. vanilla GANs will handle it with BCEWithLogitsLoss.
        """
        super(GANLoss, self).__init__()
        self.register_buffer('real_label', torch.tensor(target_real_label))
        self.register_buffer('fake_label', torch.tensor(target_fake_label))
        self.loss = nn.MSELoss()

    def get_target_tensor(self, prediction, target_is_real):
        """Create label tensors with the same size as the input.
        Parameters:
            prediction (tensor) - - tpyically the prediction from a discriminator
            target_is_real (bool) - - if the ground truth label is for real images or fake images
        Returns:
            A label tensor filled with ground truth label, and with the size of the input
        """

        if target_is_real:
            target_tensor = self.real_label
        else:
            target_tensor = self.fake_label
        return target_tensor.expand_as(prediction)

    def __call__(self, predictions,
                 target_is_real: bool,
                 do_smooth: bool=False,
                 mask: Union[None, torch.Tensor] = None):
        """Calculate loss given Discriminator's output and grount truth labels.
        Parameters:
            predictions (tensor list) - - tpyically the prediction output from a discriminator
            target_is_real (bool) - - if the ground truth label is for real images or fake images
        Returns:
            the calculated loss.
        """
        all_losses = []
        for prediction in predictions:
            target_tensor = self.get_target_tensor(prediction, target_is_real)
            if do_smooth:
                noise = (torch.rand(1) - 0.5) / 2.0  # [-0.25, 0.25]
                target_tensor = target_tensor + noise.to(target_tensor.device)
            if mask is not None:
                mask_down = F.interpolate(mask, size=prediction.shape[2:])
                prediction = prediction * mask_down
                target_tensor = target_tensor * mask_down
            loss = self.loss(prediction, target_tensor)
            all_losses.append(loss)
        return sum(all_losses)


def get_icn_inputs(planes: np.ndarray, sketch_normal: np.ndarray, sketch_mask: np.ndarray,
                   central_crop: np.ndarray, icn_w: int, icn_h: int):
    """
    Crop the original data, resize them to ICN input shape and combine them to get the ICN input.
    """

    # First we need to crop (using mask area) and resize to (icn_w, icn_h)
    ys, xs = np.nonzero(sketch_mask)
    x_min, x_max = np.min(xs), np.max(xs)
    y_min, y_max = np.min(ys), np.max(ys)

    sketch_normal_crop, crop_xy_min, pad_xy_before, pad_xy_after, _, _ = \
        square_crop_from_bbox(sketch_normal, [x_min, y_min, x_max, y_max])

    crop_info = {
        'crop_xy_min': crop_xy_min,
        'pad_xy_before': pad_xy_before,
        'pad_xy_after': pad_xy_after,
        'crop_size_orig': sketch_normal_crop.shape[:2]
    }
    sketch_normal_crop = cv2.resize(sketch_normal_crop, (icn_w, icn_h))

    planes_cropped = []
    for i in range(len(planes)):
        crop = square_crop_from_bbox(planes[i], [x_min, y_min, x_max, y_max])[0]
        crop = cv2.resize(crop, (icn_w, icn_h))
        planes_cropped.append(crop)

    planes_cropped = np.stack(planes_cropped)

    # NET INPUTS ARE IN LAB SPACE!!
    sketch_normal_crop = Normalize(mean=[0.5] * 3, std=[0.5] * 3)(
        ToTensor()((Image.fromarray(cv2.cvtColor(sketch_normal_crop, cv2.COLOR_RGB2LAB)))))

    central_crop = Normalize(mean=[0.5] * 3, std=[0.5] * 3)(
        ToTensor()((Image.fromarray(cv2.cvtColor(central_crop, cv2.COLOR_RGB2LAB)))))

    planes_cropped = planes_to_torch(planes_cropped, to_LAB=True)
    n_planes = len(planes_cropped)
    planes_cropped = planes_cropped.reshape(1, n_planes * 3, icn_h, icn_w)

    gen_in = torch.cat([sketch_normal_crop.unsqueeze(0), central_crop.unsqueeze(0), planes_cropped], dim=1)

    return gen_in, crop_info
